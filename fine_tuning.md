# AI 모델, 파라미터, 파인튜닝에 대한 이해

## AI 모델 분류

현대 AI 모델은 크게 세 가지로 분류할 수 있습니다:

1. **전통적인 ML 모델**
   - 파라미터 수: 수천~수십만 개
   - 예시: SVM, Random Forest, 간단한 신경망
   - 특징: 계산 요구사항 낮음, 특정 작업에 특화됨

2. **트랜스포머 기반 모델**
   - 파라미터 수: 수백만~수억 개
   - 예시: BERT(1.1억), RoBERTa(3억), Albert(1,300만)
   - 특징: 자기 주의(Self-Attention) 메커니즘 사용, 뛰어난 문맥 이해 능력

3. **대규모 언어 모델(LLM)**
   - 파라미터 수: 수십억~수조 개
   - 예시: GPT-3(1,750억), GPT-4(1조 이상 추정)
   - 특징: 창발적 능력(Emergent Abilities) 발현, 다양한 작업 가능

## 파라미터란?

AI 분야에서 파라미터는 크게 두 종류로 나뉩니다:

### 1. 모델 파라미터 (가중치)
- 모델이 학습 과정에서 스스로 조정하는 값들
- 예: 선형 회귀에서 y = wx + b의 w(가중치)와 b(편향)
- 모델의 표현력과 크기를 결정
- 파라미터 수가 많을수록 더 복잡한 패턴을 학습 가능

### 2. 하이퍼 파라미터
- 모델 학습 과정을 제어하는 설정값
- 사용자가 직접 설정하는 값들
- 예: 학습률, 배치 크기, 에폭 수 등

## 딥러닝과 파인튜닝

**딥러닝**은 처음부터 많은 데이터를 학습해서 스스로 패턴과 개념을 익히는 과정입니다.

**파인튜닝**은 이미 학습된 모델(지식)을 특정 목적에 맞게 조금 조정하는 과정입니다.

파인튜닝 과정에서는:
- 모델의 전체 파라미터가 메모리에 로드됨
- 기존 파라미터 값이 새로운 데이터에 맞게 미세하게 조정됨
- 일부 계층만 학습시키는 방법으로 메모리 사용량 줄일 수 있음

## Cross-Encoder 파인튜닝 특성

Cross-Encoder는 일반 인코더와 달리 (쿼리, 문서) 쌍을 완전히 새로 처리해야 하는 구조입니다:

- 매 추론마다 새로운 계산 필요 (효율성 떨어짐)
- 역전파(backpropagation) 과정에서 중간 값들을 모두 저장해야 함
- 이로 인해 일반 인코더 모델보다 더 많은 메모리 사용 (약 2배)

## 본 프로젝트에서 사용한 모델

- **모델명**: bongsoo/albert-small-kor-cross-encoder-v1
- **유형**: 트랜스포머 기반 Cross-Encoder 모델
- **파라미터 수**: 약 13M(1,300만 개)
- **특징**: 한국어에 최적화된 Cross-Encoder, 문장 쌍의 관련성 판단


## 파인튜닝 하이퍼파라미터 설정

Full Fine-tuning으로 진행하였음

```python
training_args = TrainingArguments(
    output_dir=output_dir,            # 학습 중 체크포인트와 모델이 저장될 디렉토리 경로
    num_train_epochs=epochs,          # 전체 데이터셋에 대해 학습할 에포크(반복) 수
    per_device_train_batch_size=4,    # GPU/CPU당 학습 배치 크기 (메모리 사용량 관리를 위해 작게 설정)
    per_device_eval_batch_size=4,     # 평가 시 배치 크기
    gradient_accumulation_steps=4,    # 그래디언트를 누적하는 스텝 수 (실질적으로 배치 크기를 4×4=16으로 늘리는 효과)
    warmup_steps=50,                  # 학습률을 서서히 증가시키는 워밍업 스텝 수 (초기 학습 안정화)
    weight_decay=0.01,                # 가중치 정규화 계수 (과적합 방지)
    logging_dir=f"{output_dir}/logs", # 로그 파일 저장 경로
    logging_steps=5,                  # 로깅 간격 (5스텝마다 로그 기록)
    evaluation_strategy="steps",      # 평가 전략 (스텝 단위로 평가)
    eval_steps=50,                    # 50스텝마다 평가 수행
    save_strategy="steps",            # 모델 저장 전략 (스텝 단위)
    save_steps=50,                    # 50스텝마다 모델 저장
    load_best_model_at_end=True,      # 학습 종료 시 가장 성능이 좋은 모델 로드
    metric_for_best_model="loss",     # 최적 모델 선택 기준 (손실값)
    greater_is_better=False,          # 낮은 손실값이 더 좋음을 의미
    fp16=True,                        # 16비트 부동소수점 학습 활성화 (메모리 절약 및 학습 속도 향상)
    dataloader_num_workers=0,         # 데이터 로딩 프로세스 수 (0은 메인 프로세스만 사용)
    disable_tqdm=False,               # 진행 상황 표시줄 활성화
)
```

## 메모리 사용량이 높은 이유

1. **모델 파라미터**: 1,300만 개의 파라미터가 모두 메모리에 로드됨
2. **옵티마이저 상태**: 각 파라미터마다 옵티마이저 상태 정보 저장 (Adam 옵티마이저의 경우 파라미터당 2개의 추가 값)
3. **그래디언트**: 역전파 과정에서 각 파라미터의 그래디언트 저장
4. **활성화값**: 역전파를 위해 중간 계층의 활성화값 유지
5. **배치 데이터**: 입력 데이터의 토큰화된 표현 저장
6. **Cross-Encoder 구조**: 쿼리와 문서를 함께 처리하는 구조로 인한 추가 메모리 사용

## 메모리 사용량 최적화 방법

1. **그래디언트 누적(gradient accumulation)**: 작은 배치 사용 후 그래디언트 누적 (본 프로젝트에서 사용)
2. **혼합 정밀도 학습(mixed precision training)**: FP16 사용 (본 프로젝트에서 사용)
3. **파라미터 효율적 미세 조정(PEFT)**: LoRA, Adapter 등을 사용해 일부 파라미터만 업데이트
4. **선택적 미세 조정**: 특정 레이어만 학습 가능하게 설정
5. **더 작은 기본 모델 선택**: 작은 규모의 사전학습 모델 선택

## 파인튜닝과 검색 재랭킹 과정

본 프로젝트에서는 2단계 검색 파이프라인을 구현했습니다:

1. **1차 검색(Bi-Encoder)**: 벡터 유사도 기반으로 빠르게 후보군 선별
2. **재랭킹(Cross-Encoder)**: 파인튜닝된 Cross-Encoder 모델로 후보군을 더 정확하게 재정렬

이 접근법은 효율성(Bi-Encoder의 속도)과 정확성(Cross-Encoder의 정밀도)의 균형을 이루는 방식입니다.